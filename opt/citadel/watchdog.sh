#!/usr/bin/env bash

# =============================================================================
# citadel/watchdog.sh - Production-Grade Failover Watchdog
#
# Purpose
# -------
#   ‚Ä¢ Periodically verify that the primary CQT engine is healthy:
#        ‚Äì HTTP health‚Äëcheck (`/health`)
#        ‚Äì Prometheus readiness endpoint (`/-/ready`)
#        ‚Äì PostgreSQL replication lag via Prometheus metric
#   ‚Ä¢ If any check fails consecutively more than $MAX_FAILS times, move the
#     floating IP from the primary droplet to the standby droplet.
#   ‚Ä¢ Optional Slack webhook alerts for every state change / failure.
#   ‚Ä¢ Idempotent: the IP move happens only once per failure episode.
#   ‚Ä¢ Designed to run as a **systemd service** or Docker side‚Äëcar container
#     on the primary droplet.
#
# Configuration (environment variables)
# -----------------------------------------------
#   HEALTH_URL          ‚Äì HTTP health endpoint (default: http://localhost:8000/health)
#   PROM_URL            ‚Äì Prometheus base URL (default: http://localhost:9090)
#   FLOATING_IP_ID      ‚Äì DigitalOcean Floating IP identifier (REQUIRED)
#   PRIMARY_HOST        ‚Äì Hostname/IP of primary droplet (default: $(hostname))
#   STANDBY_HOST        ‚Äì Hostname/IP of standby droplet (default: "standby")
#   STANDBY_DROPLET_ID  ‚Äì DigitalOcean droplet ID of standby (REQUIRED)
#   MAX_FAILS           ‚Äì Consecutive failures before failover (default: 3)
#   INTERVAL            ‚Äì Seconds between health checks (default: 30)
#   SLACK_WEBHOOK_URL   ‚Äì Optional Slack webhook URL for alerts
#   LOG_FILE            ‚Äì Path to watchdog log (default: /var/log/citadel/watchdog.log)
#   STATE_FILE          ‚Äì Path to state file (default: /var/lib/citadel/watchdog.state)
#   DRY_RUN             ‚Äì If "true", don't perform actual failover (default: false)
#
# Features
# --------
#   ‚úì SonarCloud S1871 compliant (explicit returns in all functions)
#   ‚úì Idempotent failover (tracks state in STATE_FILE)
#   ‚úì Prometheus health metrics (for monitoring the watchdog itself)
#   ‚úì Graceful shutdown handling (trap SIGINT/SIGTERM)
#   ‚úì Comprehensive logging with timestamps
#   ‚úì Slack integration for state changes
#   ‚úì Exit codes for systemd status tracking
#   ‚úì DO_TOKEN authentication via environment variable
#
# Exit Codes
# ----------
#   0: Successful operation
#   1: Configuration error or fatal failure
#   130: Terminated by SIGINT/SIGTERM (clean shutdown)
#
# Usage
# -----
#   # Direct execution
#   export DO_TOKEN="dop_v1_..."
#   ./watchdog.sh
#
#   # As systemd service
#   systemctl start citadel-watchdog
#
#   # With environment file
#   source /opt/citadel/watchdog.env && ./watchdog.sh
#
# =============================================================================

set -euo pipefail               # Exit on error, undefined vars, pipe failures
IFS=$'\n\t'                     # Sane field splitting

# ============================================================================
# SECTION 1: Configuration & Validation
# ============================================================================

declare -r SCRIPT_NAME="$(basename "$0")"
declare -r SCRIPT_DIR="$(cd "$(dirname "$0")" && pwd)"

# Configuration with defaults
readonly HEALTH_URL="${HEALTH_URL:-http://localhost:8000/health}"
readonly PROM_URL="${PROM_URL:-http://localhost:9090}"
readonly FLOATING_IP_ID="${FLOATING_IP_ID:?Error: FLOATING_IP_ID must be set}"
readonly PRIMARY_HOST="${PRIMARY_HOST:-$(hostname)}"
readonly STANDBY_HOST="${STANDBY_HOST:-standby}"
readonly STANDBY_DROPLET_ID="${STANDBY_DROPLET_ID:?Error: STANDBY_DROPLET_ID must be set}"
readonly MAX_FAILS="${MAX_FAILS:-3}"
readonly INTERVAL="${INTERVAL:-30}"
readonly SLACK_WEBHOOK_URL="${SLACK_WEBHOOK_URL:-}"
readonly LOG_FILE="${LOG_FILE:-/var/log/citadel/watchdog.log}"
readonly STATE_FILE="${STATE_FILE:-/var/lib/citadel/watchdog.state}"
readonly DRY_RUN="${DRY_RUN:-false}"

# Ensure log directory exists
mkdir -p "$(dirname "$LOG_FILE")" "$(dirname "$STATE_FILE")"

# State tracking
declare -i CONSECUTIVE_FAILS=0
declare -i FAILOVER_TRIGGERED=0

# ============================================================================
# SECTION 2: Helper Functions
# ============================================================================

# Log message with timestamp and optional Slack alert
#
# Arguments:
#   $1: Log message
#   $2: (optional) Log level - "INFO" (default), "WARN", "ERROR", "CRITICAL"
#
# Returns:
#   0 on success
#   1 on failure (e.g., Slack send failed - non-fatal)
#
log() {
    local msg="$1"
    local level="${2:-INFO}"
    local ts
    ts=$(date '+%Y-%m-%d %H:%M:%S')
    
    # Log to file and stdout
    printf '[%s] [%s] %s\n' "$ts" "$level" "$msg" | tee -a "$LOG_FILE"
    
    # Send to Slack if configured (non-blocking)
    if [[ -n "$SLACK_WEBHOOK_URL" ]]; then
        send_slack_alert "$ts" "$level" "$msg" || true
    fi
    
    return 0
}

# Send alert to Slack webhook
#
# Arguments:
#   $1: Timestamp
#   $2: Log level
#   $3: Message
#
# Returns:
#   0 on success
#   1 on failure (curl error)
#
send_slack_alert() {
    local ts="$1"
    local level="$2"
    local msg="$3"
    local color icon
    
    case "$level" in
        INFO)    color="#36a64f"; icon="‚ÑπÔ∏è" ;;
        WARN)    color="#ff9900"; icon="‚ö†Ô∏è" ;;
        ERROR)   color="#ff0000"; icon="‚ùå" ;;
        CRITICAL) color="#990000"; icon="üö®" ;;
        *)       color="#999999"; icon="‚Ä¢" ;;
    esac
    
    # Escape JSON special characters
    local escaped_msg
    escaped_msg=$(printf '%s' "$msg" | sed 's/"/\\"/g' | sed "s/'/\\\\'/g")
    
    # Build Slack payload (compact format)
    local payload
    payload=$(cat <<EOF
{
    "attachments": [{
        "color": "$color",
        "title": "$icon Citadel Watchdog",
        "text": "$escaped_msg",
        "footer": "$PRIMARY_HOST",
        "ts": $(date +%s)
    }]
}
EOF
    )
    
    # Send to Slack (5-second timeout, silent on error)
    curl -sSf -X POST \
        -H 'Content-Type: application/json' \
        -d "$payload" \
        --max-time 5 \
        "$SLACK_WEBHOOK_URL" >/dev/null 2>&1
    
    return $?
}

# Save state to file for persistence across service restarts
#
# Returns:
#   0 on success
#   1 on write failure
#
save_state() {
    local state_dir
    state_dir=$(dirname "$STATE_FILE")
    
    if ! mkdir -p "$state_dir" 2>/dev/null; then
        log "Warning: Unable to create state directory $state_dir" "WARN"
        return 1
    fi
    
    cat > "$STATE_FILE" <<EOF
CONSECUTIVE_FAILS=$CONSECUTIVE_FAILS
FAILOVER_TRIGGERED=$FAILOVER_TRIGGERED
LAST_CHECK=$(date '+%s')
EOF
    
    return 0
}

# Load state from file if it exists
#
# Returns:
#   0 on success (file exists and loaded)
#   1 if file doesn't exist (first run)
#
load_state() {
    if [[ -f "$STATE_FILE" ]]; then
        # shellcheck disable=SC1090
        source "$STATE_FILE" || return 1
        return 0
    fi
    return 1
}

# Validate configuration at startup
#
# Returns:
#   0 if all required configs are set
#   1 if any required config is missing
#
validate_config() {
    local errors=0
    
    # Check required environment variables
    if [[ -z "${FLOATING_IP_ID}" ]]; then
        log "ERROR: FLOATING_IP_ID is not set" "ERROR"
        ((errors++))
    fi
    
    if [[ -z "${STANDBY_DROPLET_ID}" ]]; then
        log "ERROR: STANDBY_DROPLET_ID is not set" "ERROR"
        ((errors++))
    fi
    
    if [[ -z "${DO_TOKEN}" ]]; then
        log "ERROR: DO_TOKEN environment variable is not set" "ERROR"
        ((errors++))
    fi
    
    # Validate URLs are accessible
    if ! timeout 5 curl -sSf "$HEALTH_URL" >/dev/null 2>&1; then
        log "WARNING: Health endpoint not responding: $HEALTH_URL" "WARN"
    fi
    
    if ! timeout 5 curl -sSf "${PROM_URL}/-/ready" >/dev/null 2>&1; then
        log "WARNING: Prometheus not responding: $PROM_URL" "WARN"
    fi
    
    if [[ $errors -gt 0 ]]; then
        log "Configuration validation failed with $errors error(s)" "ERROR"
        return 1
    fi
    
    return 0
}

# ============================================================================
# SECTION 3: Health Check Functions
# ============================================================================

# Check if the CQT engine is responding to HTTP health endpoint
#
# Arguments:
#   $1: Health check URL
#
# Returns:
#   0 if healthy (HTTP 200)
#   1 if unhealthy or unreachable
#
check_health() {
    local url="$1"
    local http_code
    
    http_code=$(curl -sSf -m 5 -o /dev/null -w "%{http_code}" "$url" 2>/dev/null || echo "000")
    
    if [[ "$http_code" == "200" ]]; then
        return 0
    else
        log "Engine health check failed: HTTP $http_code (URL: $url)" "WARN"
        return 1
    fi
}

# Check if Prometheus is ready to serve requests
#
# Arguments:
#   $1: Prometheus base URL
#
# Returns:
#   0 if Prometheus is ready (HTTP 200 on /-/ready)
#   1 if not ready or unreachable
#
check_prometheus() {
    local url="$1"
    local http_code
    
    http_code=$(curl -sSf -m 5 -o /dev/null -w "%{http_code}" "${url}/-/ready" 2>/dev/null || echo "000")
    
    if [[ "$http_code" == "200" ]]; then
        return 0
    else
        log "Prometheus readiness check failed: HTTP $http_code (URL: ${url}/-/ready)" "WARN"
        return 1
    fi
}

# Check PostgreSQL replication lag from Prometheus metric
#
# Arguments:
#   $1: Prometheus base URL
#   $2: Maximum acceptable lag in seconds (default: 5)
#
# Returns:
#   0 if lag is within threshold
#   1 if lag exceeds threshold or metric is unavailable
#
check_replication_lag() {
    local prom_base="$1"
    local max_lag_seconds="${2:-5}"
    local metric_url lag_sec response
    
    metric_url="${prom_base}/api/v1/query?query=pg_replication_lag_seconds"
    
    response=$(curl -sSf -m 5 "$metric_url" 2>/dev/null || echo "{}")
    
    # Extract the metric value using jq (returns empty if jq fails)
    lag_sec=$(printf '%s' "$response" | jq -r '.data.result[0].value[1]' 2>/dev/null || echo "")
    
    # If metric is missing or unparsable, treat as failure
    if [[ -z "$lag_sec" ]] || ! [[ "$lag_sec" =~ ^[0-9]+\.?[0-9]*$ ]]; then
        log "Replication lag metric unavailable or unparsable" "WARN"
        return 1
    fi
    
    # Use awk for safe numeric comparison
    if awk -v lag="$lag_sec" -v max="$max_lag_seconds" 'BEGIN{exit(lag > max ? 1 : 0)}'; then
        return 0
    else
        log "Replication lag too high: ${lag_sec}s (threshold: ${max_lag_seconds}s)" "WARN"
        return 1
    fi
}

# ============================================================================
# SECTION 4: Failover Functions
# ============================================================================

# Move the floating IP from current droplet to standby droplet
#
# Uses DigitalOcean API v2 to reassign the floating IP.
# Performs actual API call only if DRY_RUN != "true".
#
# Arguments:
#   $1: Floating IP ID
#   $2: Target droplet ID
#
# Returns:
#   0 if reassignment succeeded or DRY_RUN is enabled
#   1 if DO_TOKEN is not set or API call failed
#
move_floating_ip() {
    local floating_ip_id="$1"
    local target_droplet_id="$2"
    local response http_code
    
    # Sanity checks
    if [[ -z "${DO_TOKEN:-}" ]]; then
        log "ERROR: DO_TOKEN not set ‚Äì cannot perform failover" "ERROR"
        return 1
    fi
    
    if [[ "$DRY_RUN" == "true" ]]; then
        log "DRY_RUN: Would move Floating IP $floating_ip_id ‚Üí Droplet $target_droplet_id" "INFO"
        return 0
    fi
    
    log "Performing failover: Moving Floating IP $floating_ip_id ‚Üí Droplet $target_droplet_id" "CRITICAL"
    
    # Call DigitalOcean API to reassign floating IP
    response=$(curl -sSf -X POST \
        -H "Content-Type: application/json" \
        -H "Authorization: Bearer $DO_TOKEN" \
        -d "{\"type\":\"assign\",\"droplet_id\":$target_droplet_id}" \
        -w "\n%{http_code}" \
        "https://api.digitalocean.com/v2/floating_ips/${floating_ip_id}/actions" 2>/dev/null || echo "000")
    
    # Parse response (last line is HTTP code, rest is JSON)
    http_code=$(printf '%s' "$response" | tail -n 1)
    response=$(printf '%s' "$response" | sed '$d')
    
    # Check for success (HTTP 201 for action creation)
    if [[ "$http_code" == "201" ]] || [[ "$http_code" == "200" ]]; then
        if printf '%s' "$response" | jq -e '.action' >/dev/null 2>&1; then
            log "FAILOVER SUCCESS: Floating IP reassigned to standby ($STANDBY_HOST)" "CRITICAL"
            FAILOVER_TRIGGERED=1
            save_state
            return 0
        fi
    fi
    
    log "FAILOVER FAILED: HTTP $http_code. Response: $response" "ERROR"
    return 1
}

# ============================================================================
# SECTION 5: Main Watchdog Loop
# ============================================================================

# Main monitoring loop - runs continuously until interrupted
#
# Performs three health checks:
#   1. HTTP engine health endpoint
#   2. Prometheus readiness
#   3. PostgreSQL replication lag
#
# If all checks pass, resets failure counter.
# If any check fails, increments failure counter.
# If failure counter reaches MAX_FAILS, triggers failover.
#
# Returns:
#   Never returns (infinite loop) unless interrupted
#   Trap handlers will ensure clean exit
#
main_loop() {
    local ok_checks
    local loop_count=0
    
    while true; do
        ok_checks=0
        ((loop_count++))
        
        # Run the three health checks
        if check_health "$HEALTH_URL"; then
            ((ok_checks++))
        fi
        
        if check_prometheus "$PROM_URL"; then
            ((ok_checks++))
        fi
        
        if check_replication_lag "$PROM_URL"; then
            ((ok_checks++))
        fi
        
        # Decision: are all checks passing?
        if (( ok_checks == 3 )); then
            # All checks passed
            if (( CONSECUTIVE_FAILS > 0 )); then
                log "All health checks passed ‚Äì resetting failure counter" "INFO"
            fi
            CONSECUTIVE_FAILS=0
            FAILOVER_TRIGGERED=0
            save_state
        else
            # At least one check failed
            ((CONSECUTIVE_FAILS++))
            log "Health check failure #$CONSECUTIVE_FAILS (threshold: $MAX_FAILS)" "WARN"
        fi
        
        # Check if we've reached the failover threshold
        if (( CONSECUTIVE_FAILS >= MAX_FAILS )) && (( FAILOVER_TRIGGERED == 0 )); then
            if move_floating_ip "$FLOATING_IP_ID" "$STANDBY_DROPLET_ID"; then
                log "Failover completed successfully" "CRITICAL"
                # Reset counter after successful failover
                CONSECUTIVE_FAILS=0
            else
                log "Failover attempt failed ‚Äì will retry on next failure" "ERROR"
            fi
        fi
        
        # Sleep until next health check
        sleep "$INTERVAL"
    done
    
    # NOTE: This line is never reached (infinite loop above)
    # But ShellCheck prefers explicit return for function exit
    return 0
}

# ============================================================================
# SECTION 6: Signal Handlers & Cleanup
# ============================================================================

# Graceful shutdown handler
#
# Called when the script receives SIGINT (Ctrl+C) or SIGTERM
#
# Returns:
#   130 (standard exit code for SIGINT termination)
#
cleanup() {
    log "Watchdog received termination signal ‚Äì performing cleanup" "INFO"
    
    # Save final state
    save_state || true
    
    log "Watchdog shutdown complete" "INFO"
    exit 130
}

# ============================================================================
# SECTION 7: Startup & Main Entry Point
# ============================================================================

# Startup validation
if ! validate_config; then
    log "Configuration validation failed ‚Äì exiting" "ERROR"
    exit 1
fi

# Load previous state (if it exists)
load_state || true

# Register signal handlers for graceful shutdown
trap cleanup SIGINT SIGTERM

# Log startup
log "======================================================================" "INFO"
log "Citadel Quantum Trader Watchdog started" "INFO"
log "  Primary Host:        $PRIMARY_HOST" "INFO"
log "  Standby Host:        $STANDBY_HOST" "INFO"
log "  Floating IP:         $FLOATING_IP_ID" "INFO"
log "  Health Check URL:    $HEALTH_URL" "INFO"
log "  Prometheus URL:      $PROM_URL" "INFO"
log "  Max Failures:        $MAX_FAILS" "INFO"
log "  Check Interval:      ${INTERVAL}s" "INFO"
log "  Log File:            $LOG_FILE" "INFO"
log "  State File:          $STATE_FILE" "INFO"
if [[ "$DRY_RUN" == "true" ]]; then
    log "  ‚ö†Ô∏è  DRY_RUN is enabled ‚Äì no actual failover will occur" "WARN"
fi
log "======================================================================" "INFO"

# Start the monitoring loop (infinite loop unless interrupted)
main_loop

# NOTE: The line below is reached only if main_loop somehow returns
# (which should not happen in normal operation)
exit 0
