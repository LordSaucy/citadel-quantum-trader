global:
  resolve_timeout: 5m

route:
  receiver: 'default'
  group_by: ['alertname', 'severity']
  group_wait: 30s
  group_interval: 5m
  repeat_interval: 4h

  routes:
    # Critical alerts go to PagerDuty (or Slack if you prefer)
    - match:
        severity: critical
      receiver: pagerduty

    # Warnings go to Slack
    - match:
        severity: warning
      receiver: slack

    # Everything else (info) goes to Telegram
    - receiver: telegram

receivers:
  # ------------------- Slack -------------------
  - name: slack
    slack_configs:
      - api_url: '<SLACK_WEBHOOK_URL>'
        channel: '#citadel-alerts'
        send_resolved: true
        title: '{{ .CommonAnnotations.summary }}'
        text: |
          *Alert:* {{ .CommonAnnotations.summary }}
          *Details:* {{ .CommonAnnotations.description }}
          *Severity:* {{ .Labels.severity }}
          *Instance:* {{ .Labels.instance }}

  # ------------------- PagerDuty -------------------
  - name: pagerduty
    pagerduty_configs:
      - service_key: '<PAGERDUTY_INTEGRATION_KEY>'
        severity: '{{ .Labels.severity }}'
        send_resolved: true
        details:
          bucket: '{{ .Labels.bucket_id }}'
          drawdown: '{{ .Value }}'

  # ------------------- Telegram -------------------
  - name: telegram
    telegram_configs:
      - bot_token: '<TELEGRAM_BOT_TOKEN>'
        chat_id: '<TELEGRAM_CHAT_ID>'
        message: |
          *{{ .CommonAnnotations.summary }}*
          {{ .CommonAnnotations.description }}
          Severity: {{ .Labels.severity }}
          Bucket: {{ .Labels.bucket_id }}
        parse_mode: Markdown
        disable_notification: false
        send_resolved: true


receivers:
  - name: slack
    slack_configs:
      - api_url: '{{ file "/etc/alertmanager/slack_webhook" }}'
        ...
  - name: pagerduty
    pagerduty_configs:
      - service_key: '{{ file "/etc/alertmanager/pagerduty_key" }}'
        ...
  - name: telegram
    telegram_configs:
      - bot_token: '{{ file "/etc/alertmanager/telegram_token" }}'
        ...
        
receivers:
  - name: slack-notifications
    slack_configs:
      - api_url: '{{ env "SLACK_WEBHOOK_URL" }}'
        channel: '#trading-alerts'
        send_resolved: true
        title: '[{{ .Status }}] {{ .GroupLabels.alertname }}'
        text: |
          {{ range .Alerts }}
          *Alert:* {{ .Annotations.summary }}
          *Details:* {{ .Annotations.description }}
          *Value:* {{ .Value }}
          {{ end }}

alertmanager:
  image: prom/alertmanager:latest
  volumes:
    - ./alertmanager/alertmanager.yml:/etc/alertmanager/config.yml:ro
    - ./alertmanager/secrets:/etc/alertmanager/secrets:ro   # contains SLACK_WEBHOOK_URL, PAGERDUTY_INTEGRATION_KEY
  environment:
    - SLACK_WEBHOOK_URL=${SLACK_WEBHOOK_URL}
    - PAGERDUTY_INTEGRATION_KEY=${PAGERDUTY_INTEGRATION_KEY}
  ports:
    - "9093:9093"
  restart: unless-stopped

groups:
  - name: citadel-guards
    rules:
      # -------------------------------------------------
      # Sentiment Guard – reject too‑biased news
      # -------------------------------------------------
      - alert: SentimentGuardReject
        expr: sentiment_guard_hits_total > 0
        for: 1m
        labels:
          severity: warning
        annotations:
          summary: "News‑sentiment guard rejected a trade"
          description: "Sentiment score exceeded configured thresholds ({{ $labels.reason }})"

      # -------------------------------------------------
      # Calendar Lock‑out – high‑impact macro window active
      # -------------------------------------------------
      - alert: CalendarLockoutActive
        expr: calendar_lockout_active == 1
        for: 1m
        labels:
          severity: info
        annotations:
          summary: "Calendar lock‑out is active"
          description: "A high‑impact macro event (e.g., CPI, FOMC) is within the configured margin."

      # -------------------------------------------------
      # Volatility Spike Guard
      # -------------------------------------------------
      - alert: VolatilitySpikeDetected
        expr: volatility_spike_guard_hits_total > 0
        for: 1m
        labels:
          severity: warning
        annotations:
          summary: "Volatility spike guard rejected a trade"
          description: "Current ATR exceeded {{ $labels.atrs }} × baseline."

      # -------------------------------------------------
      # Shock Detector – spread, desync, liquidity
      # -------------------------------------------------
      - alert: ShockDetectorSpread
        expr: shock_detector_hits_total{type="spread"} > 0
        for: 30s
        labels:
          severity: warning
        annotations:
          summary: "Spread explosion detected"
          description: "Current spread > {{ $labels.multiplier }} × average spread."

      - alert: ShockDetectorDesync
        expr: shock_detector_hits_total{type="desync"} > 0
        for: 30s
        labels:
          severity: warning
        annotations:
          summary: "Quote desync detected"
          description: "Market data timestamp older than allowed latency."

      - alert: ShockDetectorLiquidity
        expr: shock_detector_hits_total{type="liquidity"} > 0
        for: 30s
        labels:
          severity: warning
        annotations:
          summary: "Liquidity vacuum detected"
          description: "LIR > {{ $labels.threshold }} or depth below {{ $labels.min_depth }}."

          receivers:
  - name: kill-switch
    webhook_configs:
      - url: http://citadel-bot:8000/api/v1/kill_switch   # internal network address
        send_resolved: true

global:
  resolve_timeout: 5m

route:
  receiver: slack-notifications
  group_by: ['alertname', 'instance']
  group_wait: 30s
  group_interval: 5m
  repeat_interval: 4h

receivers:
  - name: slack-notifications
    slack_configs:
      - api_url: https://hooks.slack.com/services/XXX/YYY/ZZZ
        channel: '#citadel-alerts'
        send_resolved: true
        title: "{{ .CommonAnnotations.summary }}"
        text: |
          {{ .CommonAnnotations.description }}

# -------------------------------------------------
# Silence groups (example)
# -------------------------------------------------
inhibit_rules:
  - source_match:
      severity: "critical"
    target_match:
      severity: "warning"
    equal: ["alertname", "instance"]

receivers:
  - name: kill-switch
    webhook_configs:
      - url: http://admin-backend:8000/api/v1/kill-switch   # endpoint that sets gauge to 1
        send_resolved: true

# monitoring/alertmanager.yml
global:
  resolve_timeout: 5m

# -----------------------------------------------------------------
# 1️⃣ Define the *calendar* of market holidays (UTC)
# -----------------------------------------------------------------
time_intervals:
  # Example: US NYSE holidays for 2025 (add as many as you need)
  - name: "nyse_holiday_2025_01_01"
    time_intervals:
      - start_time: "2025-01-01T00:00:00Z"
        end_time:   "2025-01-01T23:59:59Z"
  - name: "nyse_holiday_2025_07_04"
    time_intervals:
      - start_time: "2025-07-04T00:00:00Z"
        end_time:   "2025-07-04T23:59:59Z"
  # … add the rest of your holiday list …

# -----------------------------------------------------------------
# 2️⃣ Define a *mute* interval that starts at the beginning of a holiday
#    and lasts 5 minutes (the “grace period” you asked for)
# -----------------------------------------------------------------
mute_time_intervals:
  - name: "holiday_grace_5m"
    time_intervals:
      - start_time: "00:00:00"
        end_time:   "00:05:00"
    # The above times are *relative* to the parent time_interval.
    # They will be applied for each holiday defined above.

# -----------------------------------------------------------------
# 3️⃣ Routing – attach the mute interval to the alerts you care about
# -----------------------------------------------------------------
route:
  receiver: "slack-notifications"
  # Apply the mute interval to *all* alerts that match the names below.
  # You can add more matchers (e.g., severity) if you wish.
  routes:
    - receiver: "slack-notifications"
      match:
        alertname: "drawdown"
      mute_time_intervals: ["holiday_grace_5m"]
    - receiver: "slack-notifications"
      match:
        alertname: "order_success_degradation"
      mute_time_intervals: ["holiday_grace_5m"]
    - receiver: "slack-notifications"
      match:
        alertname: "latency_high"
      mute_time_intervals: ["holiday_grace_5m"]
    # … add any other noisy alerts you want to silence during holidays …

receivers:
  - name: "slack-notifications"
    slack_configs:
      - api_url: "${SLACK_WEBHOOK}"
        channel: "#trading-alerts"
        send_resolved: true

- alert: PgDumpFailed
  expr: citadel_pg_dump_last_success == 0
  for: 5m
  labels:
    severity: critical
  annotations:
    summary: "Nightly PostgreSQL dump failed"
    description: "Check /var/log/citadel/pg_dump.log on the host. The dump job may have exited with an error."

receivers:
- name: rollback-webhook
  webhook_configs:
  - url: http://127.0.0.1:9099/rollback   # a tiny Flask app you run locally
    send_resolved: false

groups:
  - name: production
    rules:
      - alert: ProductionExpectancyDrop
        expr: (live_expectancy / on() group_left optimiser_last_fitness) < 0.90
        for: 5m
        labels:
          severity: critical
        annotations:
          summary: "Live expectancy dropped >10 % vs. last optimiser"
          description: "Investigate immediately – automatic rollback will be triggered."

- alert: LatencySLOBurnRate
  expr: (slo:latency_99th > 0.15) and (slo:latency_99th offset 1h > 0.15)
  for: 5m
  labels:
    severity: critical
  annotations:
    summary: "Latency SLO burn‑rate > 1h"
    description: "The 99th‑percentile latency has been > 150 ms for the past hour."

receivers:
  - name: slack-primary
    slack_configs:
      - api_url: https://hooks.slack.com/services/XXX/YYY/ZZZ
        channel: "#citadel-alerts"
        send_resolved: true

  - name: pagerduty
    pagerduty_configs:
      - service_key: <PD_INTEGRATION_KEY>
        severity: "{{ .CommonLabels.severity }}"

route:
  receiver: slack-primary
  routes:
    - match:
        severity: "critical"
      receiver: pagerduty

      - alert: LatencySLOBurnRate
  expr: slo:latency_error_budget_consumed > 80
  for: 5m
  labels:
    severity: warning
  annotations:
    summary: "Latency error budget > 80 % consumed"
    description: |
      The 99th‑percentile latency has been above 150 ms enough to use
      80 % of the allowed error budget in the last hour.
      Investigate the depth guard, network latency, or broker API health.

