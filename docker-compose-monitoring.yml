version: '3.9'

# =====================================================================
# CITADEL QUANTUM TRADER - MONITORING STACK
# =====================================================================
# VPS: voc-g-1c-4gb-30s (1 vCPU, 4GB RAM, 30GB SSD)
# Location: Any region (non-critical)
# Cost: $36/mo
#
# Purpose: Metrics, dashboards, alerts, backups
# Connection: Scrapes Primary/Standby/MT5 via network
#
# Containers:
#  1. Prometheus (metrics database)
#  2. Grafana (dashboards + Mission Control UI)
#  3. AlertManager (alert routing)
#  4. Backup Agent (PostgreSQL + Redis snapshots)
#  5. Node Exporter (system metrics)
#
# Design: Completely independent from trading engine
# Failure: Can restart without affecting trading
#
# =====================================================================

services:
  # ===================================================================
  # PROMETHEUS (METRICS DATABASE)
  # ===================================================================
  prometheus:
    image: prom/prometheus:latest
    container_name: cqt_prometheus_monitoring
    restart: always
    networks:
      - monitoring
    ports:
      - "9090:9090"
    environment:
      TZ: UTC
    volumes:
      - ./prometheus.yml:/etc/prometheus/prometheus.yml
      - ./prometheus_rules.yml:/etc/prometheus/rules.yml
      - prometheus_data:/prometheus
    command:
      - "--config.file=/etc/prometheus/prometheus.yml"
      - "--storage.tsdb.path=/prometheus"
      - "--storage.tsdb.retention.time=30d"
      - "--storage.tsdb.retention.size=10GB"
      - "--web.enable-lifecycle"
      - "--web.enable-admin-api"
    healthcheck:
      test: ["CMD", "wget", "--spider", "-q", "http://localhost:9090/-/healthy"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 60s
    logging:
      driver: "json-file"
      options:
        max-size: "100m"
        max-file: "3"

  # ===================================================================
  # GRAFANA (DASHBOARDS & MISSION CONTROL)
  # ===================================================================
  grafana:
    image: grafana/grafana:latest
    container_name: cqt_grafana_monitoring
    restart: always
    networks:
      - monitoring
    ports:
      - "3000:3000"
    environment:
      # ---- Server Configuration ----
      GF_SERVER_ROOT_URL: http://${MONITORING_IP}:3000
      GF_SERVER_DOMAIN: ${MONITORING_IP}
      GF_SERVER_HTTP_PORT: 3000
      GF_USERS_ALLOW_SIGN_UP: "false"
      
      # ---- Security ----
      GF_SECURITY_ADMIN_USER: admin
      GF_SECURITY_ADMIN_PASSWORD: ${GRAFANA_ADMIN_PASSWORD}
      GF_SECURITY_SECRET_KEY: ${GRAFANA_SECRET_KEY}
      
      # ---- Database ----
      GF_DATABASE_TYPE: sqlite3
      GF_DATABASE_PATH: /var/lib/grafana/grafana.db
      
      # ---- Authentication ----
      GF_AUTH_ANONYMOUS_ENABLED: "false"
      GF_AUTH_BASIC_ENABLED: "true"
      
      # ---- Plugins ----
      GF_INSTALL_PLUGINS: grafana-piechart-panel,grafana-worldmap-panel,grafana-clock-panel
      
      # ---- Logging ----
      GF_LOG_LEVEL: info
      
    volumes:
      - grafana_data:/var/lib/grafana
      - ./provisioning/dashboards:/etc/grafana/provisioning/dashboards
      - ./provisioning/datasources:/etc/grafana/provisioning/datasources
      - ./grafana_dashboards:/etc/grafana/dashboards
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:3000/api/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 60s
    depends_on:
      - prometheus
    logging:
      driver: "json-file"
      options:
        max-size: "100m"
        max-file: "3"

  # ===================================================================
  # ALERTMANAGER (ALERT ROUTING & NOTIFICATIONS)
  # ===================================================================
  alertmanager:
    image: prom/alertmanager:latest
    container_name: cqt_alertmanager
    restart: always
    networks:
      - monitoring
    ports:
      - "9093:9093"
    environment:
      TZ: UTC
    volumes:
      - ./alertmanager.yml:/etc/alertmanager/alertmanager.yml
      - alertmanager_data:/alertmanager
    command:
      - "--config.file=/etc/alertmanager/alertmanager.yml"
      - "--storage.path=/alertmanager"
      - "--log.level=info"
    healthcheck:
      test: ["CMD", "wget", "--spider", "-q", "http://localhost:9093/-/healthy"]
      interval: 30s
      timeout: 10s
      retries: 3
    logging:
      driver: "json-file"
      options:
        max-size: "50m"
        max-file: "3"

  # ===================================================================
  # BACKUP AGENT (PostgreSQL + Redis snapshots)
  # ===================================================================
  backup_agent:
    image: citadel-backup-agent:latest
    container_name: cqt_backup_agent
    restart: always
    networks:
      - monitoring
    environment:
      # ---- Primary Database Connection ----
      PRIMARY_DB_HOST: ${PRIMARY_VPS_IP}
      PRIMARY_DB_PORT: 5432
      PRIMARY_DB_USER: cqt_user
      PRIMARY_DB_PASSWORD: ${DB_PASSWORD}
      PRIMARY_DB_NAME: citadel_trades
      
      # ---- Primary Redis Connection ----
      PRIMARY_REDIS_HOST: ${PRIMARY_VPS_IP}
      PRIMARY_REDIS_PORT: 6379
      PRIMARY_REDIS_DB: 0
      
      # ---- Backup Configuration ----
      BACKUP_SCHEDULE: "0 2 * * *"  # Daily at 02:00 UTC
      BACKUP_RETENTION_DAYS: 30
      BACKUP_COMPRESSION: "gzip"
      BACKUP_ENCRYPT: "true"
      
      # ---- Object Storage Configuration ----
      S3_ENDPOINT: https://sto.${REGION}.vultrobjects.com
      S3_BUCKET: ${S3_BUCKET_NAME}
      S3_ACCESS_KEY: ${S3_ACCESS_KEY}
      S3_SECRET_KEY: ${S3_SECRET_KEY}
      S3_REGION: ${REGION}
      
      # ---- PostgreSQL Specific ----
      PGBACKUP_MODE: full
      PGBACKUP_INCLUDE_SCHEMAS: "true"
      PGBACKUP_VERBOSE: "true"
      PGBACKUP_WAL_ARCHIVING: "true"
      
      # ---- Redis Specific ----
      REDIS_SNAPSHOT_TYPE: full
      REDIS_SNAPSHOT_INTERVAL: 86400  # 24 hours
      
      # ---- Notifications ----
      SLACK_WEBHOOK: ${SLACK_WEBHOOK_URL}
      EMAIL_ALERTS: ${ALERT_EMAIL}
      ALERT_ON_SUCCESS: "false"
      ALERT_ON_FAILURE: "true"
      
      # ---- Logging ----
      LOG_LEVEL: info
      LOG_FORMAT: json
      
    volumes:
      - backup_staging:/var/lib/backup
      - ./backup_scripts:/app/scripts
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8080/health"]
      interval: 60s
      timeout: 10s
      retries: 3
      start_period: 60s
    ports:
      - "8080:8080"
    logging:
      driver: "json-file"
      options:
        max-size: "100m"
        max-file: "3"

  # ===================================================================
  # LOKI (LOG AGGREGATION - Optional)
  # ===================================================================
  loki:
    image: grafana/loki:latest
    container_name: cqt_loki
    restart: always
    networks:
      - monitoring
    ports:
      - "3100:3100"
    environment:
      TZ: UTC
    volumes:
      - ./loki-local-config.yaml:/etc/loki/local-config.yaml
      - loki_data:/loki
    command: -config.file=/etc/loki/local-config.yaml
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:3100/ready"]
      interval: 30s
      timeout: 5s
      retries: 3
    logging:
      driver: "json-file"
      options:
        max-size: "50m"
        max-file: "3"

  # ===================================================================
  # PROMTAIL (LOG SHIPPER - Optional)
  # ===================================================================
  promtail:
    image: grafana/promtail:latest
    container_name: cqt_promtail
    restart: always
    networks:
      - monitoring
    environment:
      HOSTNAME: monitoring
    volumes:
      - ./promtail-config.yaml:/etc/promtail/config.yaml
      - /var/log:/var/log:ro
      - /var/lib/docker/containers:/var/lib/docker/containers:ro
      - /var/run/docker.sock:/var/run/docker.sock
    command: -config.file=/etc/promtail/config.yaml
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:9080/ready"]
      interval: 30s
      timeout: 5s
      retries: 3
    depends_on:
      - loki
    logging:
      driver: "json-file"
      options:
        max-size: "50m"
        max-file: "2"

  # ===================================================================
  # NODE EXPORTER (System Metrics)
  # ===================================================================
  node_exporter:
    image: prom/node-exporter:latest
    container_name: cqt_node_exporter_monitoring
    restart: always
    networks:
      - monitoring
    ports:
      - "9100:9100"
    volumes:
      - /proc:/host/proc:ro
      - /sys:/host/sys:ro
      - /:/rootfs:ro
    command:
      - "--path.procfs=/host/proc"
      - "--path.sysfs=/host/sys"
      - "--collector.filesystem.mount-points-exclude=^/(sys|proc|dev|host|etc)($$|/)"
      - "--collector.netdev.device-exclude=^(veth.*)$$"
    expose:
      - "9100"
    logging:
      driver: "json-file"
      options:
        max-size: "50m"
        max-file: "2"

  # ===================================================================
  # CADVISOR (Container Metrics)
  # ===================================================================
  cadvisor:
    image: gcr.io/cadvisor/cadvisor:latest
    container_name: cqt_cadvisor
    restart: always
    networks:
      - monitoring
    ports:
      - "8081:8080"
    volumes:
      - /:/rootfs:ro
      - /var/run:/var/run:ro
      - /sys:/sys:ro
      - /var/lib/docker/:/var/lib/docker:ro
      - /dev/disk/:/dev/disk:ro
    expose:
      - "8080"
    devices:
      - /dev/kmsg
    logging:
      driver: "json-file"
      options:
        max-size: "50m"
        max-file: "2"

networks:
  monitoring:
    driver: bridge
    ipam:
      config:
        - subnet: 172.29.0.0/16

volumes:
  prometheus_data:
    driver: local
  grafana_data:
    driver: local
  alertmanager_data:
    driver: local
  loki_data:
    driver: local
  backup_staging:
    driver: local
