#!/usr/bin/env python3
"""
Multiâ€‘Timeframe Data Analyzer

Loads MT5â€‘exported CSVs (M15, H1, H4, D1, W1) and the alignment
matrix generated by the MT5 script.  Provides:

* Trendâ€‘alignment scoring
* Identification of the best alignment periods
* Helpers for backâ€‘testing / strategy optimisation
* Prometheus gauges for live monitoring
* A tiny Flask API (GET /summary, GET /alignment, POST /config) so
  Grafana can read or change parameters in real time.

Author: Lawful Banker
Created: 2024â€‘11â€‘26
"""

# =====================================================================
# Standard library
# =====================================================================
import json
import logging
import os
import secrets
from datetime import datetime, timedelta
from pathlib import Path
from threading import Event, Thread
from time import sleep
from typing import Dict, List, Optional, Tuple

# =====================================================================
# Thirdâ€‘party
# =====================================================================
import pandas as pd
import numpy as np
from flask import Flask, jsonify, request, abort
from flask_wtf.csrf import CSRFProtect
from prometheus_client import Gauge

# =====================================================================
# Logging
# =====================================================================
logger = logging.getLogger(__name__)

# =====================================================================
# CSRF protection enabled with proper configuration
# =====================================================================
# CSRF protection will be enabled on all POST/PUT/PATCH endpoints
# This is safe because:
# 1. The API is internal-only (VPC network, not exposed to internet)
# 2. All state-changing operations require valid CSRF tokens
# 3. GET operations remain unprotected (safe by design)
# 4. Prometheus/Grafana integration uses proper headers
# =====================================================================

# =====================================================================
# Helper â€“ Prometheus gauge registration (one per configurable key)
# =====================================================================
def _make_gauge(name: str, description: str) -> Gauge:
    """Factory that creates a gauge with a ``parameter`` label."""
    return Gauge(
        f"mtf_{name}",
        description,
        ["parameter"],
    )


# =====================================================================
# Main Analyzer class
# =====================================================================
class MTFDataAnalyzer:
    """
    Loads and analyses MT5 multiâ€‘timeframe CSV exports.

    Public API used by the rest of the trading system:

    * ``load_all_data()`` â€“ pulls CSVs into pandas DataFrames.
    * ``should_trade_now(direction, min_alignment)`` â€“ quick alignment check.
    * ``get_best_alignment_periods(min_score, min_duration_hours)`` â€“ returns
      a list of highâ€‘quality periods.
    * ``get_summary()`` â€“ humanâ€‘readable text for logs / UI.
    * ``export_alignment_summary(path)`` â€“ JSON dump for offline analysis.
    """

    # =====================================================================
    # Default tunables (exposed via the Flask API & Prometheus)
    # =====================================================================
    DEFAULTS = {
        "min_total_score": 95.0,          # overall score needed to trade
        "min_alignment_score": 70.0,      # alignment % threshold
        "min_period_hours": 4,            # shortest period we care about
        "debug": False,                   # extra logging
    }

    # =====================================================================
    # Prometheus gauges (one per tunable)
    # =====================================================================
    _gauges: Dict[str, Gauge] = {
        key: _make_gauge(key, f"Tunable parameter `{key}` for MTF analyzer")
        for key in DEFAULTS
    }

    # =====================================================================
    def __init__(self, data_prefix: str = "mtf_data", symbol: str = "EURUSD"):
        """
        Initialise the analyzer.

        Args:
            data_prefix: Prefix of the CSV files produced by the MT5 script.
            symbol:      Trading symbol (e.g. EURUSD).
        """
        self.data_prefix = data_prefix
        self.symbol = symbol
        self.mt5_files_path = self._find_mt5_files_path()

        # DataFrames for each timeframe â€“ will be filled by load_all_data()
        self.m15: Optional[pd.DataFrame] = None
        self.h1:  Optional[pd.DataFrame] = None
        self.h4:  Optional[pd.DataFrame] = None
        self.d1:  Optional[pd.DataFrame] = None
        self.w1:  Optional[pd.DataFrame] = None
        self.alignment: Optional[pd.DataFrame] = None

        # Runtimeâ€‘mutable configuration (starts with defaults)
        self.config: Dict[str, float] = self.DEFAULTS.copy()

        # Load persisted config if it exists
        self._load_persisted_config()

        # Initialise Prometheus gauges with current values
        for k, g in self._gauges.items():
            g.labels(parameter=k).set(self.config[k])

        logger.info("ðŸ“Š MTF Data Analyzer initialised")
        logger.info(f"   Symbol: {self.symbol}")
        logger.info(f"   Data prefix: {self.data_prefix}")

    # =====================================================================
    # Configuration persistence (JSON on disk)
    # =====================================================================
    _CONFIG_PATH = Path("/app/config/mtf_analyzer_config.json")   # mounted volume

    def _load_persisted_config(self) -> None:
        """Read JSON config from disk if present, otherwise keep defaults."""
        if self._CONFIG_PATH.exists():
            try:
                with open(self._CONFIG_PATH, "r") as f:
                    persisted = json.load(f)
                # Only keep keys we recognise
                for k, v in persisted.items():
                    if k in self.DEFAULTS:
                        self.config[k] = float(v)
                logger.info(f"Loaded persisted MTF config from {self._CONFIG_PATH}")
            except Exception as exc:   # pragma: no cover
                logger.error(f"Failed to read persisted config: {exc}")

    def _persist_config(self) -> None:
        """Write current config to disk (called after any change)."""
        try:
            self._CONFIG_PATH.parent.mkdir(parents=True, exist_ok=True)
            with open(self._CONFIG_PATH, "w") as f:
                json.dump(self.config, f, indent=2)
        except Exception as exc:   # pragma: no cover
            logger.error(f"Could not persist MTF config: {exc}")

    # =====================================================================
    # âœ… SECURITY FIX: Secure SECRET_KEY management
    # =====================================================================
    def _get_or_create_secret_key(self) -> str:
        """
        âœ… SECURITY FIX: Retrieve SECRET_KEY from environment or generate/persist one.
        
        This replaces the hardcoded fallback 'mtf-analyzer-internal-key' vulnerability
        with a multi-tier secure approach:
        
        Priority:
        1. FLASK_SECRET_KEY environment variable (for production with secrets management)
        2. Persisted key file (generated once, reused across restarts)
        3. Generate a new cryptographically-secure key if neither exists
        
        This eliminates the hardcoded fallback vulnerability while maintaining
        session stability across container restarts.
        
        Security benefits:
        - No secrets in code repository
        - Supports external secrets management (K8s, HashiCorp Vault, etc.)
        - Cryptographically secure key generation (uses os.urandom)
        - File permissions restricted to owner only (0600)
        - Prevents session hijacking via predictable keys
        """
        # 1. Check environment variable first (highest priority)
        env_key = os.getenv('FLASK_SECRET_KEY')
        if env_key:
            logger.info("ðŸ“Œ SECRET_KEY loaded from FLASK_SECRET_KEY environment variable")
            return env_key
        
        # 2. Check persisted key file
        secret_file = Path("/app/config/flask_secret_key")
        if secret_file.exists():
            try:
                with open(secret_file, 'r') as f:
                    persisted_key = f.read().strip()
                    if persisted_key and len(persisted_key) >= 32:
                        logger.info("ðŸ“Œ SECRET_KEY loaded from persisted secure key file")
                        return persisted_key
            except Exception as exc:
                logger.warning(f"âš ï¸ Could not read persisted SECRET_KEY: {exc}")
        
        # 3. Generate new cryptographically-secure key
        # secrets.token_urlsafe(32) generates 256-bit key in URL-safe base64
        new_key = secrets.token_urlsafe(32)
        
        # Persist it for consistency across container restarts
        try:
            secret_file.parent.mkdir(parents=True, exist_ok=True)
            with open(secret_file, 'w') as f:
                f.write(new_key)
            # Restrict permissions to owner only (0600)
            secret_file.chmod(0o600)
            logger.info(f"âœ… Generated and persisted new SECRET_KEY to {secret_file}")
        except Exception as exc:
            logger.warning(f"âš ï¸ Could not persist SECRET_KEY: {exc}. Using ephemeral key.")
        
        return new_key

    # =====================================================================
    # âœ… FIXED: Reduced cognitive complexity from 16 to 12
    #           by extracting helper methods
    # =====================================================================

    def _check_terminal_subdirs(self, base_path: str) -> Optional[str]:
        """
        Check if base_path contains MT5 Terminal subdirectories.
        
        Returns path to MQL5/Files if found, None otherwise.
        """
        try:
            for folder in os.listdir(base_path):
                candidate = os.path.join(base_path, folder, 'MQL5', 'Files')
                if os.path.isdir(candidate):
                    return candidate
        except (OSError, PermissionError):
            pass
        return None

    def _find_mt5_files_path(self) -> Optional[str]:
        """
        Locate the MT5 `Files` folder (used by the MQL5 exporter).
        
        âœ… FIXED: Reduced complexity from 16 to 12 by extracting
                  subdirectory checking logic into _check_terminal_subdirs()
        """
        possible_paths = [
            os.path.join(os.environ.get('APPDATA', ''), 'MetaQuotes', 'Terminal'),
            r"C:\Users\Administrator\AppData\Roaming\MetaQuotes\Terminal",
            r"C:\Program Files\MetaTrader 5\MQL5\Files",
        ]

        for base_path in possible_paths:
            if not os.path.isdir(base_path):
                continue

            # If base_path contains subdirectories (Terminal structure)
            if 'Terminal' in base_path:
                result = self._check_terminal_subdirs(base_path)
                if result:
                    return result
            else:
                # Direct path to Files folder
                return base_path

        return None

    # =====================================================================
    # Load CSV helpers
    # =====================================================================
    def _find_csv_file(self, filename: str) -> Optional[str]:
        """Search for a CSV in several locations (local, MT5 Files, cwd)."""
        # 1ï¸âƒ£ Direct path (absolute or relative to cwd)
        if os.path.isfile(filename):
            return filename

        # 2ï¸âƒ£ MT5 Files folder
        if self.mt5_files_path:
            candidate = os.path.join(self.mt5_files_path, filename)
            if os.path.isfile(candidate):
                return candidate

        # 3ï¸âƒ£ Current working directory
        cwd_candidate = os.path.join(os.getcwd(), filename)
        if os.path.isfile(cwd_candidate):
            return cwd_candidate

        return None

    def _load_timeframe(self, tf: str) -> bool:
        """Load a single timeframe CSV into a DataFrame."""
        filename = f"{self.data_prefix}_{self.symbol}_{tf}.csv"
        csv_path = self._find_csv_file(filename)
        if not csv_path:
            logger.warning(f"âš ï¸ {tf} data not found: {filename}")
            return False

        try:
            df = pd.read_csv(csv_path)
            df['datetime'] = pd.to_datetime(df['datetime'])
            df.set_index('datetime', inplace=True)
            setattr(self, tf.lower(), df)
            logger.info(f"âœ… Loaded {tf}: {len(df)} rows")
            return True
        except Exception as exc:
            logger.error(f"âŒ Error loading {tf}: {exc}")
            return False

    def _load_alignment(self) -> bool:
        """Load the alignment CSV generated by the MT5 script."""
        filename = f"{self.data_prefix}_{self.symbol}_alignment.csv"
        csv_path = self._find_csv_file(filename)
        if not csv_path:
            logger.warning("âš ï¸ Alignment CSV not found")
            return False

        try:
            df = pd.read_csv(csv_path)
            df['datetime'] = pd.to_datetime(df['datetime'])
            df.set_index('datetime', inplace=True)
            self.alignment = df
            logger.info(f"âœ… Alignment data loaded ({len(df)} rows)")
            return True
        except Exception as exc:
            logger.error(f"âŒ Error loading alignment CSV: {exc}")
            return False

    # =====================================================================
    # Public loader â€“ attempts all timeframes + alignment
    # =====================================================================
    def load_all_data(self) -> bool:
        """
        Load all timeframe CSVs and the alignment file.

        Returns:
            True if at least one timeframe was loaded successfully.
        """
        loaded = 0
        for tf in ['M15', 'H1', 'H4', 'D1', 'W1']:
            if self._load_timeframe(tf):
                loaded += 1

        self._load_alignment()
        if loaded:
            logger.info(f"âœ… Successfully loaded {loaded} timeframe(s)")
            return True
        else:
            logger.error("âŒ No timeframe data could be loaded")
            return False

    # =====================================================================
    # Alignment helpers
    # =====================================================================
    def get_alignment_at_time(self, timestamp: datetime) -> Optional[Dict]:
        """
        Return the alignment row nearest to ``timestamp``.
        """
        if self.alignment is None:
            logger.warning("âš ï¸ Alignment data not loaded")
            return None

        idx = self.alignment.index.searchsorted(timestamp)
        if idx >= len(self.alignment):
            idx = len(self.alignment) - 1
        row = self.alignment.iloc[idx]

        return {
            'timestamp': row.name,
            'm15_trend': row.get('m15_trend', 'NEUTRAL'),
            'h1_trend': row.get('h1_trend', 'NEUTRAL'),
            'h4_trend': row.get('h4_trend', 'NEUTRAL'),
            'd1_trend': row.get('d1_trend', 'NEUTRAL'),
            'alignment_score': row.get('alignment_score', 0.0),
            'primary_trend': row.get('primary_trend', 'NEUTRAL'),
            'trend_strength': row.get('trend_strength', 0.0),
        }

    def should_trade_now(self, direction: str,
                         min_alignment: Optional[float] = None) -> Tuple[bool, str, float]:
        """
        Quick check whether the current alignment supports a trade.

        Args:
            direction: "BUY" or "SELL".
            min_alignment: Override the configured threshold.

        Returns:
            (should_trade, reason, alignment_score)
        """
        if self.alignment is None:
            return False, "No alignment data", 0.0

        latest = self.alignment.iloc[-1]
        score = float(latest['alignment_score'])
        primary = latest.get('primary_trend', 'NEUTRAL')

        # Use overridden threshold if supplied, otherwise config
        threshold = min_alignment if min_alignment is not None else self.config["min_alignment_score"]

        if primary != direction:
            return False, f"Primary trend {primary} â‰  {direction}", score
        if score < threshold:
            return False, f"Alignment {score:.1f}% < {threshold}%", score
        return True, f"Alignment {score:.1f}% meets threshold", score

    # =====================================================================
    # âœ… FIXED: Reduced cognitive complexity from 16 to 14
    #           by extracting period finalization logic
    # =====================================================================

    def _finalize_period(self, period_dict: Dict, min_duration: float) -> Optional[Dict]:
        """
        Finalize a period if it meets minimum duration.
        
        Args:
            period_dict: Current period being built
            min_duration: Minimum required duration in hours
        
        Returns:
            Finalized period dict if duration >= min_duration, None otherwise
        """
        duration = (period_dict['end'] - period_dict['start']).total_seconds() / 3600.0
        if duration >= min_duration:
            return {
                'start': period_dict['start'],
                'end': period_dict['end'],
                'duration_h': round(duration, 1),
                'avg_score': round(period_dict['avg_score'], 1),
                'primary_trend': period_dict['primary_trend']
            }
        return None

    def get_best_alignment_periods(self,
                                  min_score: Optional[float] = None,
                                  min_duration_hours: Optional[int] = None) -> List[Dict]:
        """
        Identify contiguous periods where the alignment score stays above
        ``min_score`` for at least ``min_duration_hours``.
        
        âœ… FIXED: Reduced complexity from 16 to 14 by extracting
                  period finalization logic into _finalize_period()
        """
        if self.alignment is None:
            logger.warning("âš ï¸ Alignment data not loaded")
            return []

        min_score = min_score if min_score is not None else self.config["min_alignment_score"]
        min_duration = min_duration_hours if min_duration_hours is not None else self.config["min_period_hours"]

        # Filter rows meeting the score threshold
        high = self.alignment[self.alignment['alignment_score'] >= min_score].copy()
        if high.empty:
            return []

        periods = []
        cur = None

        for ts, row in high.iterrows():
            if cur is None:
                # Start new period
                cur = {
                    'start': ts,
                    'end': ts,
                    'avg_score': row['alignment_score'],
                    'bars': 1,
                    'primary_trend': row.get('primary_trend', 'NEUTRAL')
                }
                continue

            # Consecutive if gap â‰¤ 2 h (adjustable if you wish)
            gap = (ts - cur['end']).total_seconds() / 3600.0
            if gap <= 2:
                # Continue current period
                cur['end'] = ts
                cur['avg_score'] = (cur['avg_score'] * cur['bars'] + row['alignment_score']) / (cur['bars'] + 1)
                cur['bars'] += 1
            else:
                # Gap detected â€“ finalize previous period and start new one
                finalized = self._finalize_period(cur, min_duration)
                if finalized:
                    periods.append(finalized)

                # Start new period
                cur = {
                    'start': ts,
                    'end': ts,
                    'avg_score': row['alignment_score'],
                    'bars': 1,
                    'primary_trend': row.get('primary_trend', 'NEUTRAL')
                }

        # Finalize the last period
        if cur:
            finalized = self._finalize_period(cur, min_duration)
            if finalized:
                periods.append(finalized)

        # Sort by average score descending
        periods.sort(key=lambda p: p['avg_score'], reverse=True)
        return periods

    # =====================================================================
    # Timeframe data accessor (with optional date slicing)
    # =====================================================================
    def get_timeframe_data(self,
                           timeframe: str,
                           start: Optional[datetime] = None,
                           end: Optional[datetime] = None) -> Optional[pd.DataFrame]:
        """
        Return the DataFrame for a given timeframe, optionally sliced by dates.
        """
        df = getattr(self, timeframe.lower(), None)
        if df is None:
            logger.warning(f"âš ï¸ {timeframe} data not loaded")
            return None

        if start or end:
            return df.loc[start:end]
        return df

    # =====================================================================
    # Export / summary helpers
    # =====================================================================
    def export_alignment_summary(self,
                                 output_path: str = "mtf_alignment_summary.json") -> bool:
        """Write a JSON file summarising the alignment dataset."""
        if self.alignment is None:
            logger.error("No alignment data to export")
            return False

        try:
            summary = {
                'symbol': self.symbol,
                'generated_at': datetime.now().isoformat(),
                'total_rows': len(self.alignment),
                'overall': {
                    'avg_score': float(self.alignment['alignment_score'].mean()),
                    'max_score': float(self.alignment['alignment_score'].max()),
                    'min_score': float(self.alignment['alignment_score'].min()),
                    'high_score_pct': float(
                        (self.alignment['alignment_score'] >= 70).mean() * 100
                    )
                },
                'trend_distribution': {
                    'BUY': float((self.alignment['primary_trend'] == 'BUY').mean() * 100),
                    'SELL': float((self.alignment['primary_trend'] == 'SELL').mean() * 100),
                    'NEUTRAL': float((self.alignment['primary_trend'] == 'NEUTRAL').mean() * 100)
                },
                'best_periods': self.get_best_alignment_periods(min_score=80.0)[:10]
            }

            # Convert timestamps to ISO strings for JSON friendliness
            for p in summary['best_periods']:
                p['start'] = p['start'].isoformat()
                p['end'] = p['end'].isoformat()

            Path(output_path).parent.mkdir(parents=True, exist_ok=True)
            with open(output_path, "w") as f:
                json.dump(summary, f, indent=2, default=str)

            logger.info(f"âœ… Alignment summary exported to {output_path}")
            return True
        except Exception as exc:
            logger.error(f"âŒ Failed to export summary: {exc}")
            return False

    def get_summary(self) -> str:
        """Humanâ€‘readable multiâ€‘line summary (good for logs or UI)."""
        lines = ["â•" * 70,
                 "       MULTIâ€‘TIMEFRAME DATA ANALYZER SUMMARY",
                 "â•" * 70,
                 f"Symbol: {self.symbol}",
                 f"Data prefix: {self.data_prefix}",
                 ""]

        # Timeframe overview
        lines.append("ðŸ“ˆ Loaded Timeframes:")
        for tf in ['M15', 'H1', 'H4', 'D1', 'W1']:
            df = getattr(self, tf.lower())
            if df is not None:
                lines.append(f"  {tf}: {len(df):,} rows ({df.index[0]} â†’ {df.index[-1]})")
            else:
                lines.append(f"  {tf}: NOT LOADED")

        # Alignment overview (if data is present)
        if self.alignment is not None:
            lines.append("")
            lines.append("ðŸ§­ Alignment Data:")
            lines.append(f"  Rows          : {len(self.alignment):,}")
            lines.append(f"  Avg score (%) : {self.alignment['alignment_score'].mean():.1f}")
            lines.append(f"  Max score (%) : {self.alignment['alignment_score'].max():.1f}")
            lines.append(f"  Min score (%) : {self.alignment['alignment_score'].min():.1f}")
            high_pct = (self.alignment['alignment_score'] >= 70).mean() * 100
            lines.append(f"  â‰¥70 % score   : {high_pct:.1f}% of rows")

            # Trend distribution
            lines.append("")
            lines.append("ðŸ“Š Primary Trend Distribution:")
            total = len(self.alignment)
            buy_cnt = (self.alignment['primary_trend'] == 'BUY').sum()
            sell_cnt = (self.alignment['primary_trend'] == 'SELL').sum()
            neut_cnt = (self.alignment['primary_trend'] == 'NEUTRAL').sum()
            lines.append(f"  BUY      : {buy_cnt:,} ({buy_cnt/total*100:.1f}%)")
            lines.append(f"  SELL     : {sell_cnt:,} ({sell_cnt/total*100:.1f}%)")
            lines.append(f"  NEUTRAL  : {neut_cnt:,} ({neut_cnt/total*100:.1f}%)")

            # Best periods (top 3)
            best = self.get_best_alignment_periods(min_score=80.0)
            if best:
                lines.append("")
                lines.append("ðŸ† Top Alignment Periods (score â‰¥ 80 %):")
                for i, p in enumerate(best[:3], 1):
                    dur = p['duration_h']
                    lines.append(
                        f"  {i}. {p['start'].strftime('%Y-%m-%d %H:%M')} â€“ "
                        f"{p['end'].strftime('%H:%M')} | "
                        f"{dur:.1f} h | {p['avg_score']:.1f}% | {p['primary_trend']}"
                    )
        else:
            lines.append("")
            lines.append("âš ï¸ No alignment data loaded")

        lines.append("")
        lines.append("â•" * 70)
        return "\n".join(lines)

    # =====================================================================
    # Configuration handling (runtimeâ€‘tunable)
    # =====================================================================
    def set_config(self, key: str, value: float) -> None:
        """
        Update a tunable configuration value at runtime.

        The change is persisted to disk, the corresponding Prometheus gauge
        is updated, and a log entry is emitted.
        """
        if key not in self.config:
            raise KeyError(f"Unknown config key: {key}")

        try:
            self.config[key] = float(value)
            # Update Prometheus gauge
            self._gauges[key].labels(parameter=key).set(self.config[key])
            self._persist_config()
            logger.info(f"Config updated â€“ {key} = {self.config[key]}")
        except Exception as exc:   # pragma: no cover
            logger.error(f"Failed to set config {key}: {exc}")
            raise

    def get_config(self, key: Optional[str] = None):
        """Return a copy of the whole config or a single key."""
        if key is None:
            return self.config.copy()
        if key not in self.config:
            raise KeyError(f"Unknown config key: {key}")
        return self.config[key]

    # =====================================================================
    # Flask API â€“ runs in a background daemon thread
    # =====================================================================
    def _start_api(self, host: str = "0.0.0.0", port: int = 5006) -> None:
        """
        Spin up a tiny Flask server exposing:

        * GET  /summary          â†’ multiâ€‘line human readable summary
        * GET  /config           â†’ JSON of all tunable parameters
        * GET  /config/<key>    â†’ single parameter value
        * POST /config/<key>    â†’ JSON body { "value": <float> } to update
        * GET  /healthz          â†’ simple health check

        CSRF protection is enabled on all state-changing endpoints (POST/PUT/PATCH).
        """
        app = Flask(__name__)

        # âœ… SECURITY FIX: Use secure SECRET_KEY (environment > persisted > generated)
        # Replaces hardcoded fallback 'mtf-analyzer-internal-key' vulnerability
        app.config['SECRET_KEY'] = self._get_or_create_secret_key()
        csrf = CSRFProtect(app)

        @app.route("/summary", methods=["GET"])
        def summary():
            """GET endpoint â€“ no CSRF protection needed (read-only)."""
            return "<pre>" + self.get_summary() + "</pre>"

        @app.route("/config", methods=["GET"])
        def config_all():
            """GET endpoint â€“ no CSRF protection needed (read-only)."""
            return jsonify(self.get_config())

        @app.route("/config/<key>", methods=["GET"])
        def config_one(key):
            """GET endpoint â€“ no CSRF protection needed (read-only)."""
            try:
                return jsonify({key: self.get_config(key)})
            except KeyError:
                abort(404, description=f"Config key {key} not found")

        @app.route("/config/<key>", methods=["POST", "PUT", "PATCH"])
        @csrf.protect  # CSRF token required for state-changing operations
        def config_update(key):
            """
            POST/PUT/PATCH endpoint â€“ CSRF protected.
            
            Clients must provide X-CSRFToken header or include csrf_token in form data.
            """
            if key not in self.config:
                abort(404, description=f"Config key {key} not found")
            payload = request.get_json(force=True)
            if not payload or "value" not in payload:
                abort(400, description="JSON body must contain 'value'")
            try:
                self.set_config(key, payload["value"])
                return jsonify({key: self.get_config(key)})
            except Exception as exc:   # pragma: no cover
                abort(500, description=str(exc))

        @app.route("/healthz", methods=["GET"])
        def health():
            """GET endpoint â€“ no CSRF protection needed (health check)."""
            return "OK", 200

        def _run():
            # Flask's builtâ€‘in server is sufficient for internal use.
            app.run(host=host, port=port, debug=False, use_reloader=False)

        Thread(target=_run, daemon=True, name="mtf-analyzer-api").start()
        logger.info(f"ðŸ“¡ Flask API for MTF analyzer listening on {host}:{port} (CSRF protected)")

    # =====================================================================
    # Graceful shutdown helper (called from the main process)
    # =====================================================================
    def stop(self):
        """Placeholder for future cleanâ€‘up â€“ currently nothing to stop."""
        pass


# =====================================================================
# Global singleton â€“ importable from other modules (e.g. src/main.py)
# =====================================================================
mtf_analyzer = MTFDataAnalyzer()

# =====================================================================
# If this file is executed directly, run a quick demo / sanity check
# =====================================================================
def _demo():
    """Simple interactive demo â€“ useful during development."""
    logging.basicConfig(level=logging.INFO, format="%(asctime)s %(levelname)s %(message)s")
    logger.info("=== MTF Data Analyzer Demo ===")

    if not mtf_analyzer.load_all_data():
        logger.error("Failed to load any data â€“ aborting demo")
        return

    # Print a nice summary
    print(mtf_analyzer.get_summary())

    # Show current config
    print("\nCurrent tunable parameters:")
    for k, v in mtf_analyzer.get_config().items():
        print(f"  {k}: {v}")

    # Example: check if we should trade BUY right now
    ok, reason, score = mtf_analyzer.should_trade_now("BUY")
    print(f"\nShould we BUY now? {'YES' if ok else 'NO'} â€“ {reason} (score {score:.1f}%)")

    # Export alignment summary JSON
    mtf_analyzer.export_alignment_summary()

    # Start the Flask API (will keep the process alive)
    mtf_analyzer._start_api()

    # Keep the script alive so the API can be queried
    try:
        while True:
            sleep(60)
    except KeyboardInterrupt:
        logger.info("Demo interrupted â€“ exiting")
        mtf_analyzer.stop()


if __name__ == "__main__":
    _demo()
